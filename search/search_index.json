{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pocket TTS","text":"<p>A lightweight text-to-speech (TTS) application designed to run efficiently on CPUs. Forget about the hassle of using GPUs and web APIs serving TTS models. With Kyutai's Pocket TTS, generating audio is just a pip install and a function call away.</p> <p>Supports Python 3.10, 3.11, 3.12, 3.13 and 3.14. Requires PyTorch 2.5+. Does not require the gpu version of PyTorch.</p> <p>\ud83d\udd0a Demo | \ud83d\udc31\u200d\ud83d\udcbbGitHub Repository | \ud83e\udd17 Hugging Face Model Card | \u2699\ufe0f Tech report | \ud83d\udcc4 Paper | \ud83d\udcda Documentation</p>"},{"location":"#main-takeaways","title":"Main takeaways","text":"<ul> <li>Runs on CPU</li> <li>Small model size, 100M parameters</li> <li>Audio streaming</li> <li>Low latency, ~200ms to get the first audio chunk</li> <li>Faster than real-time, ~6x real-time on a CPU of MacBook Air M4</li> <li>Uses only 2 CPU cores</li> <li>Python API and CLI</li> <li>Voice cloning</li> <li>English only at the moment</li> <li>Can handle infinitely long text inputs</li> <li>Can run on client-side in the browser</li> </ul>"},{"location":"#trying-it-from-the-website-without-installing-anything","title":"Trying it from the website, without installing anything","text":"<p>Navigate to the Kyutai website to try it out directly in your browser. You can input text, select different voices, and generate speech without any installation.</p>"},{"location":"#trying-it-with-the-cli","title":"Trying it with the CLI","text":""},{"location":"#the-generate-command","title":"The <code>generate</code> command","text":"<p>You can use pocket-tts directly from the command line. We recommend using <code>uv</code> as it installs any dependencies on the fly in an isolated environment (uv installation instructions here). You can also use <code>pip install pocket-tts</code> to install it manually.</p> <p>This will generate a wav file <code>./tts_output.wav</code> saying the default text with the default voice, and display some speed statistics.</p> <pre><code>uvx pocket-tts generate\n# or if you installed it manually with pip:\npocket-tts generate\n</code></pre> <p>Modify the voice with <code>--voice</code> and the text with <code>--text</code>. We provide a small catalog of voices.</p> <p>You can take a look at this page which details the licenses for each voice.</p> <ul> <li>alba</li> <li>marius</li> <li>javert</li> <li>jean</li> <li>fantine</li> <li>cosette</li> <li>eponine</li> <li>azelma</li> </ul> <p>The <code>--voice</code> argument can also take a plain wav file as input for voice cloning. You can use your own or check out our voice repository. We recommend cleaning the sample before using it with Pocket TTS, because the audio quality of the sample is also reproduced.</p> <p>Feel free to check out the generate documentation for more details and examples. For trying multiple voices and prompts quickly, prefer using the <code>serve</code> command.</p>"},{"location":"#the-serve-command","title":"The <code>serve</code> command","text":"<p>You can also run a local server to generate audio via HTTP requests.</p> <pre><code>uvx pocket-tts serve\n# or if you installed it manually with pip:\npocket-tts serve\n</code></pre> <p>Navigate to <code>http://localhost:8000</code> to try the web interface, it's faster than the command line as the model is kept in memory between requests.</p> <p>You can check out the serve documentation for more details and examples.</p>"},{"location":"#the-export-voice-command","title":"The <code>export-voice</code> command","text":"<p>Processing an audio file (e.g., a .wav or .mp3) for voice cloning is relatively slow, but loading a safetensors file -- a voice embedding converted from an audio file -- is very fast. You can use the <code>export-voice</code> command to do this conversion. See the export-voice documentation for more details and examples.</p>"},{"location":"#using-it-as-a-python-library","title":"Using it as a Python library","text":"<p>You can try out the Python library on Colab here.</p> <p>Install the package with</p> <pre><code>pip install pocket-tts\n# or\nuv add pocket-tts\n</code></pre> <p>You can use this package as a simple Python library to generate audio from text.</p> <pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\n\ntts_model = TTSModel.load_model()\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"alba\"  # One of the pre-made voices, see above\n    # You can also use any voice file you have locally or from Hugging Face:\n    # \"./some_audio.wav\"\n    # or \"hf://kyutai/tts-voices/expresso/ex01-ex02_default_001_channel2_198s.wav\"\n)\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n# Audio is a 1D torch tensor containing PCM data.\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n</code></pre> <p>You can have multiple voice states around if you have multiple voices you want to use. <code>load_model()</code> and <code>get_state_for_audio_prompt()</code> are relatively slow operations, so we recommend to keep the model and voice states in memory if you can.</p> <p>You can check out the Python API documentation for more details and examples.</p>"},{"location":"#unsupported-features","title":"Unsupported features","text":"<p>At the moment, we do not support (but would love pull requests adding):</p> <ul> <li>Running the TTS inside a web browser (WebAssembly)</li> <li>A compiled version with for example <code>torch.compile()</code> or <code>candle</code>.</li> <li>Adding silence in the text input to generate pauses.</li> <li>Quantization to run the computation in int8.</li> </ul> <p>We tried running this TTS model on the GPU but did not observe a speedup compared to CPU execution, notably because we use a batch size of 1 and a very small model.</p>"},{"location":"#development-and-local-setup","title":"Development and local setup","text":"<p>We accept contributions! Feel free to open issues or pull requests on GitHub.</p> <p>You can find development instructions in the CONTRIBUTING.md file. You'll also find there how to have an editable install of the package for local development.</p>"},{"location":"#in-browser-implementations","title":"In-browser implementations","text":"<p>Pocket TTS is small enough to run directly in your browser in WebAssembly/JavaScript. We don't have official support for this yet, but you can try out one of these community implementations:</p> <ul> <li>babybirdprd/pocket-tts: Candle version (Rust) with WebAssembly and PyO3 bindings, meaning it can run on the web too.</li> <li>ekzhang/jax-js: Using jax-js, a ML library for the web. Demo here</li> <li>KevinAHM/pocket-tts-onnx-export: Model exported to .onnx and run using ONNX Runtime Web. Demo here</li> </ul>"},{"location":"#alterative-implementations","title":"Alterative implementations","text":"<ul> <li>jishnuvenugopal/pocket-tts-mlx - MLX backend optimized for Apple Silicon</li> <li>babybirdprd/pocket-tts - Candle version (Rust) with WebAssembly and PyO3 bindings.</li> </ul>"},{"location":"#projects-using-pocket-tts","title":"Projects using Pocket TTS","text":"<ul> <li>lukasmwerner/pocket-reader - Browser screen reader</li> <li>ikidd/pocket-tts-wyoming - Docker container for pocket-tts using Wyoming protocol, ready for Home Assistant Voice use.</li> <li>slaughters85j/pocket-tts - Mac Desktop App + macOS Quick Action</li> <li>teddybear082/pocket-tts-openai_streaming_server - OpenAI-compatible streaming server, dockerized and with an <code>.exe</code> release</li> </ul>"},{"location":"#prohibited-use","title":"Prohibited use","text":"<p>Use of our model must comply with all applicable laws and regulations and must not result in, involve, or facilitate any illegal, harmful, deceptive, fraudulent, or unauthorized activity. Prohibited uses include, without limitation, voice impersonation or cloning without explicit and lawful consent; misinformation, disinformation, or deception (including fake news, fraudulent calls, or presenting generated content as genuine recordings of real people or events); and the generation of unlawful, harmful, libelous, abusive, harassing, discriminatory, hateful, or privacy-invasive content. We disclaim all liability for any non-compliant use.</p>"},{"location":"#authors","title":"Authors","text":"<p>Manu Orsini, Simon Rouard, Gabriel De Marmiesse*, V\u00e1clav Volhejn, Neil Zeghidour, Alexandre D\u00e9fossez</p> <p>*equal contribution</p>"},{"location":"API%20Reference/python-api/","title":"Python API Documentation","text":"<p>Kyutai Pocket TTS provides a Python API for integrating text-to-speech capabilities into your applications.</p>"},{"location":"API%20Reference/python-api/#installation","title":"Installation","text":"<pre><code>pip install pocket-tts\n</code></pre>"},{"location":"API%20Reference/python-api/#quick-start","title":"Quick Start","text":"<pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\n\n# Load the model\ntts_model = TTSModel.load_model()\n\n# Get voice state from an audio file\nvoice_state = tts_model.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"\n)\n\n# Generate audio\naudio = tts_model.generate_audio(voice_state, \"Hello world, this is a test.\")\n\n# Save to file\nscipy.io.wavfile.write(\"output.wav\", tts_model.sample_rate, audio.numpy())\n</code></pre>"},{"location":"API%20Reference/python-api/#core-classes","title":"Core Classes","text":""},{"location":"API%20Reference/python-api/#ttsmodel","title":"TTSModel","text":"<p>The main class for text-to-speech generation.</p>"},{"location":"API%20Reference/python-api/#class-methods","title":"Class Methods","text":""},{"location":"API%20Reference/python-api/#load_modelconfigb6369a24-temp07-lsd_decode_steps1-noise_clampnone-eos_threshold-40","title":"<code>load_model(config=\"b6369a24\", temp=0.7, lsd_decode_steps=1, noise_clamp=None, eos_threshold=-4.0)</code>","text":"<p>Load and return a TTSModel instance with pre-trained weights.</p> <p>Parameters: - <code>config</code> (str): Path to model config YAML file or a variant identifier (default: \"b6369a24\") - <code>temp</code> (float): Sampling temperature for generation (default: 0.7) - <code>lsd_decode_steps</code> (int): Number of generation steps (default: 1) - <code>noise_clamp</code> (float | None): Maximum value for noise sampling (default: None) - <code>eos_threshold</code> (float): Threshold for end-of-sequence detection (default: -4.0)</p> <p>Returns: - <code>TTSModel</code>: Loaded model instance on CPU</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\n# Load with default settings\nmodel = TTSModel.load_model()\n\n# Load with custom parameters\nmodel = TTSModel.load_model(variant=\"b6369a24\", temp=0.5, lsd_decode_steps=5, eos_threshold=-3.0)\n</code></pre></p>"},{"location":"API%20Reference/python-api/#properties","title":"Properties","text":""},{"location":"API%20Reference/python-api/#device-str","title":"<code>device</code> (str)","text":"<p>Returns the device type where the model is running (\"cpu\" or \"cuda\"). By default, the model runs on CPU.</p> <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\nprint(f\"Model running on: {model.device}\")\n</code></pre>"},{"location":"API%20Reference/python-api/#sample_rate-int","title":"<code>sample_rate</code> (int)","text":"<p>Returns the generated audio sample rate (typically 24000 Hz).</p> <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\nprint(f\"Sample rate: {model.sample_rate} Hz\")\n</code></pre>"},{"location":"API%20Reference/python-api/#methods","title":"Methods","text":""},{"location":"API%20Reference/python-api/#get_state_for_audio_promptaudio_conditioning-truncatefalse","title":"<code>get_state_for_audio_prompt(audio_conditioning, truncate=False)</code>","text":"<p>Extract model state for a given audio file or URL (voice cloning), or load from a .safetensors file.</p> <p>Parameters: - <code>audio_conditioning</code> (Path | str | torch.Tensor): Audio or .safetensors file path, URL, or tensor - <code>truncate</code> (bool): Whether to truncate the audio (default: False)</p> <p>Returns: - <code>dict</code>: Model state dictionary containing hidden states and positional information</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# From local file\nvoice_state = model.get_state_for_audio_prompt(\"./my_voice.wav\")\n\n# Reload state from a .safetensors file (much faster than extracting from an audio file)\nvoice_state = model.get_state_for_audio_prompt(\"./my_voices.safetensors\")\n\n# From HTTP URL\nvoice_state = model.get_state_for_audio_prompt(\n    \"https://huggingface.co/kyutai/tts-voices/resolve\"\n    \"/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n)\n</code></pre></p>"},{"location":"API%20Reference/python-api/#generate_audiomodel_state-text_to_generate-frames_after_eosnone-copy_statetrue","title":"<code>generate_audio(model_state, text_to_generate, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate complete audio tensor from text input.</p> <p>Parameters: - <code>model_state</code> (dict): Model state from <code>get_state_for_audio_prompt()</code> - <code>text_to_generate</code> (str): Text to convert to speech - <code>frames_after_eos</code> (int | None): Frames to generate after EOS detection (default: None) - <code>copy_state</code> (bool): Whether to copy the state (default: True)</p> <p>Returns: - <code>torch.Tensor</code>: Audio 1D tensor with shape [samples]</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# Generate audio\naudio = model.generate_audio(voice_state, \"Hello world!\", frames_after_eos=2, copy_state=True)\n\nprint(f\"Generated audio shape: {audio.shape}\")\nprint(f\"Audio duration: {audio.shape[-1] / model.sample_rate:.2f} seconds\")\n</code></pre></p>"},{"location":"API%20Reference/python-api/#generate_audio_streammodel_state-text_to_generate-frames_after_eosnone-copy_statetrue","title":"<code>generate_audio_stream(model_state, text_to_generate, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate audio streaming chunks from text input.</p> <p>Parameters: Same as <code>generate_audio()</code></p> <p>Yields: - <code>torch.Tensor</code>: Audio chunks with shape [samples]</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n# Stream generation\nfor chunk in model.generate_audio_stream(voice_state, \"Long text content...\"):\n    # Process each chunk as it's generated\n    print(f\"Generated chunk: {chunk.shape[0]} samples\")\n    # Could save chunks to file or play in real-time\n</code></pre></p>"},{"location":"API%20Reference/python-api/#save_audio_promptaudio_conditioning-export_path-truncatefalse","title":"<code>save_audio_prompt(audio_conditioning, export_path, truncate=False)</code>","text":"<p>Save audio prompt to a .safetensors file.</p> <p>Parameters: - <code>audio_conditioning</code> (Path | str | torch.Tensor): Audio file path, URL, or tensor - <code>export_path</code> (Path | str): .safetensors file path - <code>truncate</code> (bool): Whether to truncate the audio (default: False)</p> <p>Returns: - tensor of the converted audio.</p> <p>Example: <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nmodel.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\", \"casual.safetensors\"\n)\n\n# From local file (the .safetensors extension will be added automatically)\ntensor = model.get_state_for_audio_prompt(\"./my_voice.wav\", \"my_voice\")\n\n# Use the tensor, Luke!\naudio = model.generate_audio(tensor, \"Hello world!\")\n</code></pre></p>"},{"location":"API%20Reference/python-api/#advanced-usage","title":"Advanced Usage","text":""},{"location":"API%20Reference/python-api/#voice-management","title":"Voice Management","text":"<pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# Preload multiple voices\nvoices = {\n    \"casual\": model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"),\n    \"funny\": model.get_state_for_audio_prompt(\n        \"https://huggingface.co/kyutai/tts-voices/resolve/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n    ),\n}\n\n# Generate with different voices\ncasual_audio = model.generate_audio(voices[\"casual\"], \"Hey there!\")\nfunny_audio = model.generate_audio(voices[\"funny\"], \"Good morning.\")\n</code></pre>"},{"location":"API%20Reference/python-api/#batch-processing","title":"Batch Processing","text":"<pre><code>from pocket_tts import TTSModel\nimport scipy.io.wavfile\nimport torch\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n# Process multiple texts efficiently by re-using the same voice state\ntexts = [\n    \"First sentence to generate.\",\n    \"Second sentence to generate.\",\n    \"Third sentence to generate.\",\n]\n\naudios = []\nfor text in texts:\n    audio = model.generate_audio(voice_state, text)\n    audios.append(audio)\n\n# Concatenate all audio\nfull_audio = torch.cat(audios, dim=0)\nscipy.io.wavfile.write(\"batch_output.wav\", model.sample_rate, full_audio.numpy())\n</code></pre>"},{"location":"API%20Reference/python-api/#streaming-to-file","title":"Streaming to File","text":"<p>You can refer to our CLI implementation which can stream audio to a wav file.</p> <p>For more information about the command-line interface, see the Generate Documentation or Serve Documentation.</p>"},{"location":"API%20Reference/Reference/tts_model/","title":"Tts model","text":""},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel","title":"<code>pocket_tts.models.tts_model.TTSModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>class TTSModel(nn.Module):\n    _TOKENS_PER_SECOND_ESTIMATE = 3.0\n    _GEN_SECONDS_PADDING = 2.0\n\n    def __init__(\n        self,\n        flow_lm: FlowLMModel,\n        temp: float,\n        lsd_decode_steps: int,\n        noise_clamp: float | None,\n        eos_threshold,\n        config: Config,\n    ):\n        super().__init__()\n        self.flow_lm = flow_lm\n        self.temp = temp\n        self.lsd_decode_steps = lsd_decode_steps\n        self.noise_clamp = noise_clamp\n        self.eos_threshold = eos_threshold\n        self.config = config\n        self.has_voice_cloning = True\n\n    @property\n    def device(self) -&gt; str:\n        return next(self.parameters()).device.type\n\n    @property\n    def sample_rate(self) -&gt; int:\n        return self.config.mimi.sample_rate\n\n    @classmethod\n    def _from_pydantic_config(\n        cls, config: Config, temp, lsd_decode_steps, noise_clamp: float | None, eos_threshold\n    ) -&gt; Self:\n        flow_lm = FlowLMModel.from_pydantic_config(\n            config.flow_lm, latent_dim=config.mimi.quantizer.dimension\n        )\n        tts_model = cls(flow_lm, temp, lsd_decode_steps, noise_clamp, eos_threshold, config)\n        return tts_model\n\n    @classmethod\n    def _from_pydantic_config_with_weights(\n        cls, config: Config, temp, lsd_decode_steps, noise_clamp: float | None, eos_threshold\n    ) -&gt; Self:\n        tts_model = cls._from_pydantic_config(\n            config, temp, lsd_decode_steps, noise_clamp, eos_threshold\n        )\n        tts_model.flow_lm.speaker_proj_weight = torch.nn.Parameter(\n            torch.zeros((1024, 512), dtype=torch.float32)\n        )\n        if config.flow_lm.weights_path is not None:\n            if config.mimi.weights_path is None:\n                raise ValueError(\n                    \"If you specify flow_lm.weights_path you should specify mimi.weights_path\"\n                )\n            logger.info(f\"Loading FlowLM weights from {config.flow_lm.weights_path}\")\n            state_dict_flowlm = get_flow_lm_state_dict(\n                download_if_necessary(config.flow_lm.weights_path)\n            )\n            tts_model.flow_lm.load_state_dict(state_dict_flowlm, strict=True)\n\n        # safetensors.torch.save_file(tts_model.state_dict(), \"7442637a.safetensors\")\n        # Create mimi config directly from the provided config using model_dump\n        mimi_config = config.mimi.model_dump()\n\n        # Build mimi model from config\n        encoder = SEANetEncoder(**mimi_config[\"seanet\"])\n        decoder = SEANetDecoder(**mimi_config[\"seanet\"])\n\n        encoder_transformer = mimi_transformer.ProjectedTransformer(**mimi_config[\"transformer\"])\n        decoder_transformer = mimi_transformer.ProjectedTransformer(**mimi_config[\"transformer\"])\n        quantizer = DummyQuantizer(**mimi_config[\"quantizer\"])\n\n        tts_model.mimi = MimiModel(\n            encoder,\n            decoder,\n            quantizer,\n            channels=mimi_config[\"channels\"],\n            sample_rate=mimi_config[\"sample_rate\"],\n            frame_rate=mimi_config[\"frame_rate\"],\n            encoder_frame_rate=mimi_config[\"sample_rate\"] / encoder.hop_length,\n            encoder_transformer=encoder_transformer,\n            decoder_transformer=decoder_transformer,\n        ).to(device=\"cpu\")\n\n        # Load mimi weights from the config safetensors file with complete mapping for strict loading\n\n        if config.mimi.weights_path is not None:\n            if config.flow_lm.weights_path is None:\n                raise ValueError(\n                    \"If you specify mimi.weights_path you should specify flow_lm.weights_path\"\n                )\n            logger.info(f\"Loading Mimi weights from {config.mimi.weights_path}\")\n            mimi_state = get_mimi_state_dict(download_if_necessary(config.mimi.weights_path))\n            tts_model.mimi.load_state_dict(mimi_state, strict=True)\n\n        tts_model.mimi.eval()\n        # tts_model.to(dtype=torch.float32)\n\n        # uncomment to save the weights\n        # tts_model = tts_model.to(dtype=torch.bfloat16)\n        # safetensors.torch.save_file(tts_model.state_dict(), \"tts_b6369a24.safetensors\")\n        if config.weights_path is not None:\n            logger.info(f\"Loading TTSModel weights from {config.weights_path}\")\n            try:\n                weights_file = download_if_necessary(config.weights_path)\n            except Exception:\n                tts_model.has_voice_cloning = False\n                weights_file = download_if_necessary(config.weights_path_without_voice_cloning)\n\n            state_dict = safetensors.torch.load_file(weights_file)\n            tts_model.load_state_dict(state_dict, strict=True)\n\n        if config.flow_lm.weights_path is None and config.weights_path is None:\n            logger.warning(\n                \"No weights_path specified for FlowLM or TTSModel, model is uninitialized!\"\n            )\n        size_in_mb = size_of_dict(tts_model.state_dict()) // 1e6\n        logging.info(f\"TTS Model loaded successfully. Its size is {size_in_mb} MB\")\n\n        return tts_model\n\n    @classmethod\n    def load_model(\n        cls,\n        config: str | Path = DEFAULT_VARIANT,\n        temp: float | int = DEFAULT_TEMPERATURE,\n        lsd_decode_steps: int = DEFAULT_LSD_DECODE_STEPS,\n        noise_clamp: float | int | None = DEFAULT_NOISE_CLAMP,\n        eos_threshold: float = DEFAULT_EOS_THRESHOLD,\n    ) -&gt; Self:\n        \"\"\"Load a pre-trained TTS model with specified configuration.\n\n        This class method loads a complete TTS model including the flow language model\n        and Mimi compression model from pre-trained weights. The model is initialized\n        with the specified generation parameters and ready for inference.\n\n        Args:\n            config: a path to a custom YAML config file saved locally (e.g., C://pocket_tts/pocket_tts_config.yaml)\n                or a model variant identifier (e.g., '610b0b2c'; must match a YAML file in the config directory).\n            temp: Sampling temperature for generation. Higher values produce more\n                diverse but potentially lower quality output.\n            lsd_decode_steps: Number of steps for Lagrangian Self Distillation\n                decoding. More steps can improve quality but increase computation.\n            noise_clamp: Maximum value for noise sampling. If None, no clamping\n                is applied. Helps prevent extreme values in generation.\n            eos_threshold: Threshold for end-of-sequence detection. Higher values\n                make the model more likely to continue generating.\n\n        Returns:\n            TTSModel: Fully initialized model with loaded weights on cpu, ready for\n                text-to-speech generation.\n\n        Raises:\n            FileNotFoundError: If the specified config file or model weights\n                are not found.\n            ValueError: If the configuration is invalid or incompatible.\n\n        Example:\n            ```python\n            from pocket_tts import TTSModel\n\n            # Load with default settings\n            model = TTSModel.load_model()\n\n            # Load with custom parameters\n            model = TTSModel.load_model(variant=\"b6369a24\", temp=0.5, lsd_decode_steps=5, eos_threshold=-3.0)\n            ```\n        \"\"\"\n        if str(config).endswith(\".yaml\"):\n            config_path = Path(config)\n            config = load_config(config_path)\n            logger.info(f\"Loading model from config at {config_path}...\")\n        else:\n            config = load_config(Path(__file__).parents[1] / f\"config/{config}.yaml\")\n\n        tts_model = TTSModel._from_pydantic_config_with_weights(\n            config, temp, lsd_decode_steps, noise_clamp, eos_threshold\n        )\n        return tts_model\n\n    def _run_flow_lm_and_increment_step(\n        self,\n        model_state: dict,\n        text_tokens: torch.Tensor | None = None,\n        backbone_input_latents: torch.Tensor | None = None,\n        audio_conditioning: torch.Tensor | None = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"First one is the backbone output, second one is the audio decoding output.\"\"\"\n        if text_tokens is None:\n            text_tokens = torch.zeros((1, 0), dtype=torch.int64, device=self.flow_lm.device)\n        if backbone_input_latents is None:\n            backbone_input_latents = torch.empty(\n                (1, 0, self.flow_lm.ldim), dtype=self.flow_lm.dtype, device=self.flow_lm.device\n            )\n        if audio_conditioning is None:\n            audio_conditioning = torch.empty(\n                (1, 0, self.flow_lm.dim), dtype=self.flow_lm.dtype, device=self.flow_lm.device\n            )\n\n        output = self._run_flow_lm(\n            text_tokens=text_tokens,\n            backbone_input_latents=backbone_input_latents,\n            model_state=model_state,\n            audio_conditioning=audio_conditioning,\n        )\n        increment_by = (\n            text_tokens.shape[1] + backbone_input_latents.shape[1] + audio_conditioning.shape[1]\n        )\n        increment_steps(self.flow_lm, model_state, increment=increment_by)\n        return output\n\n    def _run_flow_lm(\n        self,\n        model_state: dict,\n        text_tokens: torch.Tensor,\n        backbone_input_latents: torch.Tensor,\n        audio_conditioning: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        text_embeddings = self.flow_lm.conditioner(TokenizedText(text_tokens))\n        text_embeddings = torch.cat([text_embeddings, audio_conditioning], dim=1)\n\n        output_embeddings, is_eos = self.flow_lm._sample_next_latent(\n            backbone_input_latents,\n            text_embeddings,\n            model_state=model_state,\n            lsd_decode_steps=self.lsd_decode_steps,\n            temp=self.temp,\n            noise_clamp=self.noise_clamp,\n            eos_threshold=self.eos_threshold,\n        )\n        return output_embeddings[:, None, :], is_eos\n\n    def _encode_audio(self, audio: torch.Tensor) -&gt; torch.Tensor:\n        encoded = self.mimi.encode_to_latent(audio)\n        latents = encoded.transpose(-1, -2).to(torch.float32)\n        conditioning = F.linear(latents, self.flow_lm.speaker_proj_weight)\n        return conditioning\n\n    def _expand_kv_cache(self, model_state: dict, sequence_length: int) -&gt; None:\n        \"\"\"Expand KV cache back to full sequence_length for generation.\n\n        When a model state is retrieved from cache with sliced KV caches,\n        this method expands them back to the full size needed for generation.\n\n        Args:\n            model_state: The model state dict containing potentially sliced KV caches\n            sequence_length: Target sequence length to expand caches to\n        \"\"\"\n        for module_name, module_state in model_state.items():\n            if \"cache\" in module_state:\n                cache = module_state[\"cache\"]\n                # KV cache has shape [2, batch_size, current_length, num_heads, dim_per_head]\n                current_length = cache.shape[2]\n                if current_length &lt; sequence_length:\n                    # Create expanded cache filled with NaN for unused positions\n                    expanded_cache = torch.full(\n                        (\n                            cache.shape[0],\n                            cache.shape[1],\n                            sequence_length,\n                            cache.shape[3],\n                            cache.shape[4],\n                        ),\n                        float(\"NaN\"),\n                        device=cache.device,\n                        dtype=cache.dtype,\n                    )\n                    # Copy existing data to the beginning\n                    expanded_cache[:, :, :current_length, :, :] = cache\n                    module_state[\"cache\"] = expanded_cache\n\n    def _flow_lm_current_end(self, model_state: dict) -&gt; int:\n        for module_state in model_state.values():\n            current_end = module_state.get(\"current_end\")\n            if current_end is not None:\n                return int(current_end.shape[0])\n        raise ValueError(\n            \"Could not find current_end in model state, please open an issue \"\n            \"at https://github.com/kyutai-labs/pocket-tts/issues\"\n        )\n\n    @torch.no_grad\n    def _decode_audio_worker(self, latents_queue: queue.Queue, result_queue: queue.Queue):\n        \"\"\"Worker thread function for decoding audio latents from queue with immediate streaming.\"\"\"\n        try:\n            audio_chunks = []\n            mimi_context = self.config.mimi.transformer.context\n            mimi_state = init_states(self.mimi, batch_size=1, sequence_length=mimi_context)\n            while True:\n                latent = latents_queue.get()\n                if latent is None:\n                    break\n                mimi_decoding_input = latent * self.flow_lm.emb_std + self.flow_lm.emb_mean\n                transposed = mimi_decoding_input.transpose(-1, -2)\n                quantized = self.mimi.quantizer(transposed)\n\n                t = time.monotonic()\n                audio_frame = self.mimi.decode_from_latent(quantized, mimi_state)\n                increment_steps(self.mimi, mimi_state, increment=16)\n                audio_frame_duration = audio_frame.shape[2] / self.config.mimi.sample_rate\n                # We could log the timings here.\n                logger.debug(\n                    \" \" * 30 + \"Decoded %d ms of audio with mimi in %d ms\",\n                    int(audio_frame_duration * 1000),\n                    int((time.monotonic() - t) * 1000),\n                )\n                audio_chunks.append(audio_frame)\n\n                result_queue.put((\"chunk\", audio_frame))\n\n                latents_queue.task_done()\n\n            # Signal completion\n            result_queue.put((\"done\", None))\n\n        except Exception as e:\n            # Put error in result queue\n            result_queue.put((\"error\", e))\n\n    @torch.no_grad\n    def generate_audio(\n        self,\n        model_state: dict,\n        text_to_generate: str,\n        max_tokens: int = MAX_TOKEN_PER_CHUNK,\n        frames_after_eos: int | None = None,\n        copy_state: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generate complete audio tensor from text input.\n\n        This method generates the full audio output for the given text prompt\n        and returns it as a single tensor. It internally uses the streaming\n        generation method but collects all chunks before returning.\n\n        This method is NOT thread-safe; separate model instances should be used\n        for concurrent generation.\n\n        Args:\n            model_state: Model state dictionary containing hidden states and\n                positional information. Can be obtained from get_state_for_audio_prompt()\n                or init_states(). The state may be modified during generation.\n            text_to_generate: Input text to convert to speech. The text will be\n                automatically formatted (capitalization, punctuation) for optimal\n                generation quality.\n            frames_after_eos: Number of additional frames to generate after\n                detecting end-of-sequence. If None, automatically determined\n                based on text length (1-3 frames).\n            copy_state: Whether to create a deep copy of the model state before\n                generation. If True, preserves the original state for reuse.\n                If False, modifies the input state in-place. Defaults to True.\n\n        Returns:\n            torch.Tensor: Generated audio tensor with shape [channels, samples]\n                at the model's sample rate (typically 24kHz). The audio is\n                normalized and ready for playback or saving.\n                You can get the sample rate from the `sample_rate` attribute.\n\n        Raises:\n            ValueError: If text_to_generate is empty or invalid.\n            RuntimeError: If generation fails due to model errors.\n\n        Example:\n            ```python\n            from pocket_tts import TTSModel\n\n            model = TTSModel.load_model()\n\n            voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n            # Generate audio\n            audio = model.generate_audio(voice_state, \"Hello world!\", frames_after_eos=2, copy_state=True)\n\n            print(f\"Generated audio shape: {audio.shape}\")\n            print(f\"Audio duration: {audio.shape[-1] / model.sample_rate:.2f} seconds\")\n            ```\n        \"\"\"\n        audio_chunks = []\n        for chunk in self.generate_audio_stream(\n            model_state=model_state,\n            text_to_generate=text_to_generate,\n            frames_after_eos=frames_after_eos,\n            copy_state=copy_state,\n            max_tokens=max_tokens,\n        ):\n            audio_chunks.append(chunk)\n        return torch.cat(audio_chunks, dim=0)\n\n    @torch.no_grad\n    def generate_audio_stream(\n        self,\n        model_state: dict,\n        text_to_generate: str,\n        max_tokens: int = MAX_TOKEN_PER_CHUNK,\n        frames_after_eos: int | None = None,\n        copy_state: bool = True,\n    ):\n        \"\"\"Generate audio streaming chunks from text input.\n\n        This method generates audio from text and yields chunks as they become\n        available, enabling real-time playback or processing. It uses multithreading\n        to parallelize generation and decoding for optimal performance.\n        This method is NOT thread-safe; separate model instances should be used\n        for concurrent generation.\n\n        Args:\n            model_state: Model state dictionary containing hidden states and\n                positional information. Can be obtained from get_state_for_audio_prompt()\n                or init_states(). The state may be modified during generation.\n            text_to_generate: Input text to convert to speech. The text will be\n                automatically formatted (capitalization, punctuation) for optimal\n                generation quality.\n            frames_after_eos: Number of additional frames to generate after\n                detecting end-of-sequence. If None, automatically determined\n                based on text length (1-3 frames). Defaults to None.\n            copy_state: Whether to create a deep copy of the model state before\n                generation. If True, preserves the original state for reuse.\n                If False, modifies the input state in-place. Defaults to True.\n\n        Yields:\n            torch.Tensor: Audio chunks with shape [samples] at the model's\n                sample rate (typically 24kHz). Chunks are yielded as soon as\n                they are decoded, enabling real-time streaming.\n\n        Raises:\n            ValueError: If text_to_generate is empty or invalid.\n            RuntimeError: If generation fails due to model errors or threading issues.\n\n        Example:\n            ```python\n            from pocket_tts import TTSModel\n\n            model = TTSModel.load_model()\n\n            voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n            # Stream generation\n            for chunk in model.generate_audio_stream(voice_state, \"Long text content...\"):\n                # Process each chunk as it's generated\n                print(f\"Generated chunk: {chunk.shape[0]} samples\")\n                # Could save chunks to file or play in real-time\n            ```\n\n        Note:\n            This method uses multithreading to parallelize latent generation\n            and audio decoding. Generation performance is logged including\n            real-time factor (RTF) metrics.\n        \"\"\"\n\n        # This is a very simplistic way of handling long texts. We could do much better\n        # by using teacher forcing, but it would be a bit slower.\n        # TODO: add the teacher forcing method for long texts where we use the audio of one chunk\n        # as conditioning for the next chunk.\n        chunks = split_into_best_sentences(\n            self.flow_lm.conditioner.tokenizer, text_to_generate, max_tokens\n        )\n\n        for chunk in chunks:\n            text_to_generate, frames_after_eos_guess = prepare_text_prompt(chunk)\n            frames_after_eos_guess += 2\n            effective_frames = (\n                frames_after_eos if frames_after_eos is not None else frames_after_eos_guess\n            )\n            yield from self._generate_audio_stream_short_text(\n                model_state=model_state,\n                text_to_generate=chunk,\n                frames_after_eos=effective_frames,\n                copy_state=copy_state,\n            )\n\n    @torch.no_grad\n    def _generate_audio_stream_short_text(\n        self, model_state: dict, text_to_generate: str, frames_after_eos: int, copy_state: bool\n    ):\n        if copy_state:\n            model_state = copy.deepcopy(model_state)\n\n        # Set up multithreaded generation and decoding\n        latents_queue = queue.Queue()\n        result_queue = queue.Queue()\n\n        # Start decoder worker thread\n        decoder_thread = threading.Thread(\n            target=self._decode_audio_worker, args=(latents_queue, result_queue), daemon=True\n        )\n        logger.info(\"starting timer now!\")\n        t_generating = time.monotonic()\n        decoder_thread.start()\n\n        # Generate latents and add them to queue (decoder processes them in parallel)\n        self._generate(\n            model_state=model_state,\n            text_to_generate=text_to_generate,\n            frames_after_eos=frames_after_eos,\n            latents_queue=latents_queue,\n            result_queue=result_queue,\n        )\n\n        # Stream audio chunks as they become available\n        total_generated_samples = 0\n        while True:\n            result = result_queue.get()\n            if result[0] == \"chunk\":\n                # Audio chunk available immediately for streaming/playback\n                audio_chunk = result[1]\n                total_generated_samples += audio_chunk.shape[-1]\n                yield audio_chunk[0, 0]  # Remove batch, channel\n            elif result[0] == \"done\":\n                # Generation complete\n                break\n            elif result[0] == \"error\":\n                # Wait for decoder thread to finish cleanly before propagating error\n                with display_execution_time(\"Waiting for mimi decoder to finish\"):\n                    decoder_thread.join()\n                # Propagate error\n                raise result[1]\n\n        # Wait for decoder thread to finish cleanly\n        with display_execution_time(\"Waiting for mimi decoder to finish\"):\n            decoder_thread.join()\n\n        # Print timing information\n        duration_generated_audio = int(\n            total_generated_samples * 1000 / self.config.mimi.sample_rate\n        )\n        generation_time = int((time.monotonic() - t_generating) * 1000)\n        real_time_factor = duration_generated_audio / generation_time\n\n        logger.info(\n            \"Generated: %d ms of audio in %d ms so %.2fx faster than real-time\",\n            duration_generated_audio,\n            generation_time,\n            real_time_factor,\n        )\n\n    @torch.no_grad\n    def _generate(\n        self,\n        model_state: dict,\n        text_to_generate: str,\n        frames_after_eos: int,\n        latents_queue: queue.Queue,\n        result_queue: queue.Queue,\n    ):\n        prepared = self.flow_lm.conditioner.prepare(text_to_generate)\n        token_count = prepared.tokens.shape[1]\n        max_gen_len = self._estimate_max_gen_len(token_count)\n        current_end = self._flow_lm_current_end(model_state)\n        required_len = current_end + token_count + max_gen_len\n        self._expand_kv_cache(model_state, sequence_length=required_len)\n\n        with display_execution_time(\"Prompting text\"):\n            self._run_flow_lm_and_increment_step(\n                model_state=model_state, text_tokens=prepared.tokens\n            )\n\n        def run_generation():\n            try:\n                self._autoregressive_generation(\n                    model_state, max_gen_len, frames_after_eos, latents_queue\n                )\n            except Exception as e:\n                logger.error(f\"Error in autoregressive generation: {e}\")\n                # Signal decoder to stop by putting None (completion sentinel)\n                if latents_queue is not None:\n                    latents_queue.put(None)\n                # Report error to main thread\n                if result_queue is not None:\n                    result_queue.put((\"error\", e))\n\n        generation_thread = threading.Thread(target=run_generation, daemon=True)\n        generation_thread.start()\n\n    @torch.no_grad\n    def _autoregressive_generation(\n        self, model_state: dict, max_gen_len: int, frames_after_eos: int, latents_queue: queue.Queue\n    ):\n        backbone_input = torch.full(\n            (1, 1, self.flow_lm.ldim),\n            fill_value=float(\"NaN\"),\n            device=next(iter(self.flow_lm.parameters())).device,\n            dtype=self.flow_lm.dtype,\n        )\n        steps_times = []\n        eos_step = None\n        for generation_step in range(max_gen_len):\n            with display_execution_time(\"Generating latent\", print_output=False) as timer:\n                next_latent, is_eos = self._run_flow_lm_and_increment_step(\n                    model_state=model_state, backbone_input_latents=backbone_input\n                )\n                if is_eos.item() and eos_step is None:\n                    eos_step = generation_step\n                if eos_step is not None and generation_step &gt;= eos_step + frames_after_eos:\n                    break\n\n                # Add generated latent to queue for immediate decoding\n                latents_queue.put(next_latent)\n                backbone_input = next_latent\n            steps_times.append(timer.elapsed_time_ms)\n        else:\n            if os.environ.get(\"KPOCKET_TTS_ERROR_WITHOUT_EOS\", \"0\") == \"1\":\n                raise RuntimeError(\"Generation reached maximum length without EOS!\")\n            logger.warning(\n                \"Maximum generation length reached without EOS, this very often indicates an error.\"\n            )\n\n        # Add sentinel value to signal end of generation\n        latents_queue.put(None)\n        logger.info(\"Average generation step time: %d ms\", int(statistics.mean(steps_times)))\n\n    @lru_cache(maxsize=2)\n    def _cached_get_state_for_audio_prompt(\n        self, audio_conditioning: Path | str | torch.Tensor, truncate: bool = False\n    ) -&gt; dict:\n        return self.get_state_for_audio_prompt(audio_conditioning, truncate)\n\n    @torch.no_grad\n    def get_state_for_audio_prompt(\n        self, audio_conditioning: Path | str | torch.Tensor, truncate: bool = False\n    ) -&gt; dict:\n        \"\"\"Create model state conditioned on audio prompt for continuation.\n\n        This method processes an audio prompt and creates a model state that\n        captures the acoustic characteristics (speaker voice, style, prosody)\n        for use in subsequent text-to-speech generation. The resulting state\n        enables voice cloning and audio continuation with speaker consistency.\n\n        Args:\n            audio_conditioning: Audio prompt to condition (or .safetensors to load). Can be:\n                - Path: Local file path to audio file (or .safetensors)\n                - str: URL to download audio file (or .safetensors) from\n                - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]\n            truncate: Whether to truncate long audio prompts to 30 seconds.\n                Helps prevent memory issues with very long inputs. Defaults to False.\n\n        Returns:\n            dict: Model state dictionary containing hidden states and positional\n                information conditioned on the audio prompt. This state can be\n                passed to `generate_audio()` or `generate_audio_stream()` for\n                voice-consistent generation.\n\n        Raises:\n            FileNotFoundError: If audio file path doesn't exist.\n            ValueError: If audio tensor is invalid or empty.\n            RuntimeError: If audio processing or encoding fails.\n\n        Example:\n            ```python\n            from pocket_tts import TTSModel\n\n            model = TTSModel.load_model()\n            # From HuggingFace URL\n            voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n            # From local file\n            voice_state = model.get_state_for_audio_prompt(\"./my_voice.wav\")\n\n            # Reload state from a .safetensors file (much faster than extracting from an audio file)\n            voice_state = model.get_state_for_audio_prompt(\"./my_voices.safetensors\")\n\n            # From HTTP URL\n            voice_state = model.get_state_for_audio_prompt(\n                \"https://huggingface.co/kyutai/tts-voices/resolve\"\n                \"/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n            )\n            ```\n\n        Note:\n            - Audio is automatically resampled to the model's sample rate (24kHz)\n            - The audio is encoded using the Mimi compression model and projected\n              to the flow model's latent space\n            - Processing time is logged for performance monitoring\n            - The state preserves speaker characteristics for voice cloning\n        \"\"\"\n        if isinstance(audio_conditioning, (str, Path)) and str(audio_conditioning).endswith(\n            \".safetensors\"\n        ):\n            if isinstance(audio_conditioning, str):\n                audio_conditioning = download_if_necessary(audio_conditioning)\n            import safetensors.torch\n\n            prompt = safetensors.torch.load_file(audio_conditioning)[\"audio_prompt\"]\n        elif isinstance(audio_conditioning, str) and audio_conditioning in PREDEFINED_VOICES:\n            # We get the audio conditioning directly from the safetensors file.\n            prompt = load_predefined_voice(audio_conditioning)\n        else:\n            if not self.has_voice_cloning and isinstance(audio_conditioning, (str, Path)):\n                raise ValueError(VOICE_CLONING_UNSUPPORTED)\n\n            if isinstance(audio_conditioning, str):\n                audio_conditioning = download_if_necessary(audio_conditioning)\n\n            if isinstance(audio_conditioning, Path):\n                audio, conditioning_sample_rate = audio_read(audio_conditioning)\n\n                if truncate:\n                    max_samples = int(30 * conditioning_sample_rate)  # 30 seconds of audio\n                    if audio.shape[-1] &gt; max_samples:\n                        audio = audio[..., :max_samples]\n                        logger.info(f\"Audio truncated to first 30 seconds ({max_samples} samples)\")\n\n                audio_conditioning = convert_audio(\n                    audio, conditioning_sample_rate, self.config.mimi.sample_rate, 1\n                )\n\n            with display_execution_time(\"Encoding audio prompt\"):\n                prompt = self._encode_audio(audio_conditioning.unsqueeze(0).to(self.device))\n\n        model_state = init_states(self.flow_lm, batch_size=1, sequence_length=prompt.shape[1])\n\n        with display_execution_time(\"Prompting audio\"):\n            self._run_flow_lm_and_increment_step(model_state=model_state, audio_conditioning=prompt)\n\n        logger.info(\n            \"Size of the model state for audio prompt: %d MB\", size_of_dict(model_state) // 1e6\n        )\n\n        return model_state\n\n    def _estimate_max_gen_len(self, token_count: int) -&gt; int:\n        gen_len_sec = token_count / self._TOKENS_PER_SECOND_ESTIMATE + self._GEN_SECONDS_PADDING\n        frame_rate = self.config.mimi.frame_rate\n        return math.ceil(gen_len_sec * frame_rate)\n\n    @torch.no_grad\n    def save_audio_prompt(\n        self,\n        audio_conditioning: Path | str | torch.Tensor,\n        export_path: Path | str,\n        truncate: bool = False,\n    ) -&gt; torch.Tensor:\n        \"\"\"Save audio prompt to .safetensors file\n\n        This method processes an audio prompt and exports it to a .safetensors file,\n        which can be loaded by get_state_for_audio_prompt in subsequent uses\n        without converting the audio again.\n\n        It also takes an already converted audio tensor and exports it as a .safetensors file\n\n        Args:\n            audio_conditioning: Audio to export\n                - Path: Local file path to audio file\n                - str: URL to download audio file\n                - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]\n            export_path: Path to output file\n            truncate: Whether to truncate long audio prompts to 30 seconds.\n\n        Returns:\n            Audio tensor of converted audio\n\n        Raises:\n            FileNotFoundError: If audio file path doesn't exist.\n            ValueError: If audio tensor export path is invalid or empty.\n            RuntimeError: If audio processing or encoding fails.\n\n        Example:\n            ```python\n            from pocket_tts import TTSModel\n\n            model = TTSModel.load_model()\n            # From HuggingFace URL\n            model.get_state_for_audio_prompt(\n                \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\", \"casual.safetensors\"\n            )\n\n            # From local file (the .safetensors extension will be added automatically)\n            tensor = model.get_state_for_audio_prompt(\"./my_voice.wav\", \"my_voice\")\n\n            # Use the tensor, Luke!\n            audio = model.generate_audio(tensor, \"Hello world!\")\n            ```\n\n        Note:\n            - Send resulting audio tensor to get_state_for_audio_prompt\n              in order to get the model state for generation.\n        \"\"\"\n        if not export_path or not isinstance(export_path, (str, Path)):\n            raise ValueError(\"export_path must be of type str or Path\")\n        export_path = Path(export_path).with_suffix(\".safetensors\")\n\n        if not self.has_voice_cloning and isinstance(audio_conditioning, (str, Path)):\n            raise ValueError(VOICE_CLONING_UNSUPPORTED)\n\n        if isinstance(audio_conditioning, str):\n            audio_conditioning = download_if_necessary(audio_conditioning)\n\n        if isinstance(audio_conditioning, Path):\n            audio, conditioning_sample_rate = audio_read(audio_conditioning)\n\n            if truncate:\n                max_samples = int(30 * conditioning_sample_rate)  # 30 seconds of audio\n                if audio.shape[-1] &gt; max_samples:\n                    audio = audio[..., :max_samples]\n                    logger.info(f\"Audio truncated to first 30 seconds ({max_samples} samples)\")\n\n            audio_conditioning = convert_audio(\n                audio, conditioning_sample_rate, self.config.mimi.sample_rate, 1\n            )\n\n        with display_execution_time(\"Exporting audio prompt\"):\n            prompt = self._encode_audio(audio_conditioning.unsqueeze(0).to(self.device))\n            import safetensors.torch\n\n            safetensors.torch.save_file({\"audio_prompt\": prompt}, export_path)\n\n        return audio_conditioning\n</code></pre>"},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel.generate_audio","title":"<code>generate_audio(model_state, text_to_generate, max_tokens=MAX_TOKEN_PER_CHUNK, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate complete audio tensor from text input.</p> <p>This method generates the full audio output for the given text prompt and returns it as a single tensor. It internally uses the streaming generation method but collects all chunks before returning.</p> <p>This method is NOT thread-safe; separate model instances should be used for concurrent generation.</p> <p>Parameters:</p> Name Type Description Default <code>model_state</code> <code>dict</code> <p>Model state dictionary containing hidden states and positional information. Can be obtained from get_state_for_audio_prompt() or init_states(). The state may be modified during generation.</p> required <code>text_to_generate</code> <code>str</code> <p>Input text to convert to speech. The text will be automatically formatted (capitalization, punctuation) for optimal generation quality.</p> required <code>frames_after_eos</code> <code>int | None</code> <p>Number of additional frames to generate after detecting end-of-sequence. If None, automatically determined based on text length (1-3 frames).</p> <code>None</code> <code>copy_state</code> <code>bool</code> <p>Whether to create a deep copy of the model state before generation. If True, preserves the original state for reuse. If False, modifies the input state in-place. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Generated audio tensor with shape [channels, samples] at the model's sample rate (typically 24kHz). The audio is normalized and ready for playback or saving. You can get the sample rate from the <code>sample_rate</code> attribute.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_to_generate is empty or invalid.</p> <code>RuntimeError</code> <p>If generation fails due to model errors.</p> Example <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# Generate audio\naudio = model.generate_audio(voice_state, \"Hello world!\", frames_after_eos=2, copy_state=True)\n\nprint(f\"Generated audio shape: {audio.shape}\")\nprint(f\"Audio duration: {audio.shape[-1] / model.sample_rate:.2f} seconds\")\n</code></pre> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>@torch.no_grad\ndef generate_audio(\n    self,\n    model_state: dict,\n    text_to_generate: str,\n    max_tokens: int = MAX_TOKEN_PER_CHUNK,\n    frames_after_eos: int | None = None,\n    copy_state: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Generate complete audio tensor from text input.\n\n    This method generates the full audio output for the given text prompt\n    and returns it as a single tensor. It internally uses the streaming\n    generation method but collects all chunks before returning.\n\n    This method is NOT thread-safe; separate model instances should be used\n    for concurrent generation.\n\n    Args:\n        model_state: Model state dictionary containing hidden states and\n            positional information. Can be obtained from get_state_for_audio_prompt()\n            or init_states(). The state may be modified during generation.\n        text_to_generate: Input text to convert to speech. The text will be\n            automatically formatted (capitalization, punctuation) for optimal\n            generation quality.\n        frames_after_eos: Number of additional frames to generate after\n            detecting end-of-sequence. If None, automatically determined\n            based on text length (1-3 frames).\n        copy_state: Whether to create a deep copy of the model state before\n            generation. If True, preserves the original state for reuse.\n            If False, modifies the input state in-place. Defaults to True.\n\n    Returns:\n        torch.Tensor: Generated audio tensor with shape [channels, samples]\n            at the model's sample rate (typically 24kHz). The audio is\n            normalized and ready for playback or saving.\n            You can get the sample rate from the `sample_rate` attribute.\n\n    Raises:\n        ValueError: If text_to_generate is empty or invalid.\n        RuntimeError: If generation fails due to model errors.\n\n    Example:\n        ```python\n        from pocket_tts import TTSModel\n\n        model = TTSModel.load_model()\n\n        voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n        # Generate audio\n        audio = model.generate_audio(voice_state, \"Hello world!\", frames_after_eos=2, copy_state=True)\n\n        print(f\"Generated audio shape: {audio.shape}\")\n        print(f\"Audio duration: {audio.shape[-1] / model.sample_rate:.2f} seconds\")\n        ```\n    \"\"\"\n    audio_chunks = []\n    for chunk in self.generate_audio_stream(\n        model_state=model_state,\n        text_to_generate=text_to_generate,\n        frames_after_eos=frames_after_eos,\n        copy_state=copy_state,\n        max_tokens=max_tokens,\n    ):\n        audio_chunks.append(chunk)\n    return torch.cat(audio_chunks, dim=0)\n</code></pre>"},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel.generate_audio_stream","title":"<code>generate_audio_stream(model_state, text_to_generate, max_tokens=MAX_TOKEN_PER_CHUNK, frames_after_eos=None, copy_state=True)</code>","text":"<p>Generate audio streaming chunks from text input.</p> <p>This method generates audio from text and yields chunks as they become available, enabling real-time playback or processing. It uses multithreading to parallelize generation and decoding for optimal performance. This method is NOT thread-safe; separate model instances should be used for concurrent generation.</p> <p>Parameters:</p> Name Type Description Default <code>model_state</code> <code>dict</code> <p>Model state dictionary containing hidden states and positional information. Can be obtained from get_state_for_audio_prompt() or init_states(). The state may be modified during generation.</p> required <code>text_to_generate</code> <code>str</code> <p>Input text to convert to speech. The text will be automatically formatted (capitalization, punctuation) for optimal generation quality.</p> required <code>frames_after_eos</code> <code>int | None</code> <p>Number of additional frames to generate after detecting end-of-sequence. If None, automatically determined based on text length (1-3 frames). Defaults to None.</p> <code>None</code> <code>copy_state</code> <code>bool</code> <p>Whether to create a deep copy of the model state before generation. If True, preserves the original state for reuse. If False, modifies the input state in-place. Defaults to True.</p> <code>True</code> <p>Yields:</p> Type Description <p>torch.Tensor: Audio chunks with shape [samples] at the model's sample rate (typically 24kHz). Chunks are yielded as soon as they are decoded, enabling real-time streaming.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_to_generate is empty or invalid.</p> <code>RuntimeError</code> <p>If generation fails due to model errors or threading issues.</p> Example <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n# Stream generation\nfor chunk in model.generate_audio_stream(voice_state, \"Long text content...\"):\n    # Process each chunk as it's generated\n    print(f\"Generated chunk: {chunk.shape[0]} samples\")\n    # Could save chunks to file or play in real-time\n</code></pre> Note <p>This method uses multithreading to parallelize latent generation and audio decoding. Generation performance is logged including real-time factor (RTF) metrics.</p> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>@torch.no_grad\ndef generate_audio_stream(\n    self,\n    model_state: dict,\n    text_to_generate: str,\n    max_tokens: int = MAX_TOKEN_PER_CHUNK,\n    frames_after_eos: int | None = None,\n    copy_state: bool = True,\n):\n    \"\"\"Generate audio streaming chunks from text input.\n\n    This method generates audio from text and yields chunks as they become\n    available, enabling real-time playback or processing. It uses multithreading\n    to parallelize generation and decoding for optimal performance.\n    This method is NOT thread-safe; separate model instances should be used\n    for concurrent generation.\n\n    Args:\n        model_state: Model state dictionary containing hidden states and\n            positional information. Can be obtained from get_state_for_audio_prompt()\n            or init_states(). The state may be modified during generation.\n        text_to_generate: Input text to convert to speech. The text will be\n            automatically formatted (capitalization, punctuation) for optimal\n            generation quality.\n        frames_after_eos: Number of additional frames to generate after\n            detecting end-of-sequence. If None, automatically determined\n            based on text length (1-3 frames). Defaults to None.\n        copy_state: Whether to create a deep copy of the model state before\n            generation. If True, preserves the original state for reuse.\n            If False, modifies the input state in-place. Defaults to True.\n\n    Yields:\n        torch.Tensor: Audio chunks with shape [samples] at the model's\n            sample rate (typically 24kHz). Chunks are yielded as soon as\n            they are decoded, enabling real-time streaming.\n\n    Raises:\n        ValueError: If text_to_generate is empty or invalid.\n        RuntimeError: If generation fails due to model errors or threading issues.\n\n    Example:\n        ```python\n        from pocket_tts import TTSModel\n\n        model = TTSModel.load_model()\n\n        voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n        # Stream generation\n        for chunk in model.generate_audio_stream(voice_state, \"Long text content...\"):\n            # Process each chunk as it's generated\n            print(f\"Generated chunk: {chunk.shape[0]} samples\")\n            # Could save chunks to file or play in real-time\n        ```\n\n    Note:\n        This method uses multithreading to parallelize latent generation\n        and audio decoding. Generation performance is logged including\n        real-time factor (RTF) metrics.\n    \"\"\"\n\n    # This is a very simplistic way of handling long texts. We could do much better\n    # by using teacher forcing, but it would be a bit slower.\n    # TODO: add the teacher forcing method for long texts where we use the audio of one chunk\n    # as conditioning for the next chunk.\n    chunks = split_into_best_sentences(\n        self.flow_lm.conditioner.tokenizer, text_to_generate, max_tokens\n    )\n\n    for chunk in chunks:\n        text_to_generate, frames_after_eos_guess = prepare_text_prompt(chunk)\n        frames_after_eos_guess += 2\n        effective_frames = (\n            frames_after_eos if frames_after_eos is not None else frames_after_eos_guess\n        )\n        yield from self._generate_audio_stream_short_text(\n            model_state=model_state,\n            text_to_generate=chunk,\n            frames_after_eos=effective_frames,\n            copy_state=copy_state,\n        )\n</code></pre>"},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel.get_state_for_audio_prompt","title":"<code>get_state_for_audio_prompt(audio_conditioning, truncate=False)</code>","text":"<p>Create model state conditioned on audio prompt for continuation.</p> <p>This method processes an audio prompt and creates a model state that captures the acoustic characteristics (speaker voice, style, prosody) for use in subsequent text-to-speech generation. The resulting state enables voice cloning and audio continuation with speaker consistency.</p> <p>Parameters:</p> Name Type Description Default <code>audio_conditioning</code> <code>Path | str | Tensor</code> <p>Audio prompt to condition (or .safetensors to load). Can be: - Path: Local file path to audio file (or .safetensors) - str: URL to download audio file (or .safetensors) from - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]</p> required <code>truncate</code> <code>bool</code> <p>Whether to truncate long audio prompts to 30 seconds. Helps prevent memory issues with very long inputs. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Model state dictionary containing hidden states and positional information conditioned on the audio prompt. This state can be passed to <code>generate_audio()</code> or <code>generate_audio_stream()</code> for voice-consistent generation.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If audio file path doesn't exist.</p> <code>ValueError</code> <p>If audio tensor is invalid or empty.</p> <code>RuntimeError</code> <p>If audio processing or encoding fails.</p> Example <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nvoice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n# From local file\nvoice_state = model.get_state_for_audio_prompt(\"./my_voice.wav\")\n\n# Reload state from a .safetensors file (much faster than extracting from an audio file)\nvoice_state = model.get_state_for_audio_prompt(\"./my_voices.safetensors\")\n\n# From HTTP URL\nvoice_state = model.get_state_for_audio_prompt(\n    \"https://huggingface.co/kyutai/tts-voices/resolve\"\n    \"/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n)\n</code></pre> Note <ul> <li>Audio is automatically resampled to the model's sample rate (24kHz)</li> <li>The audio is encoded using the Mimi compression model and projected   to the flow model's latent space</li> <li>Processing time is logged for performance monitoring</li> <li>The state preserves speaker characteristics for voice cloning</li> </ul> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>@torch.no_grad\ndef get_state_for_audio_prompt(\n    self, audio_conditioning: Path | str | torch.Tensor, truncate: bool = False\n) -&gt; dict:\n    \"\"\"Create model state conditioned on audio prompt for continuation.\n\n    This method processes an audio prompt and creates a model state that\n    captures the acoustic characteristics (speaker voice, style, prosody)\n    for use in subsequent text-to-speech generation. The resulting state\n    enables voice cloning and audio continuation with speaker consistency.\n\n    Args:\n        audio_conditioning: Audio prompt to condition (or .safetensors to load). Can be:\n            - Path: Local file path to audio file (or .safetensors)\n            - str: URL to download audio file (or .safetensors) from\n            - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]\n        truncate: Whether to truncate long audio prompts to 30 seconds.\n            Helps prevent memory issues with very long inputs. Defaults to False.\n\n    Returns:\n        dict: Model state dictionary containing hidden states and positional\n            information conditioned on the audio prompt. This state can be\n            passed to `generate_audio()` or `generate_audio_stream()` for\n            voice-consistent generation.\n\n    Raises:\n        FileNotFoundError: If audio file path doesn't exist.\n        ValueError: If audio tensor is invalid or empty.\n        RuntimeError: If audio processing or encoding fails.\n\n    Example:\n        ```python\n        from pocket_tts import TTSModel\n\n        model = TTSModel.load_model()\n        # From HuggingFace URL\n        voice_state = model.get_state_for_audio_prompt(\"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")\n\n        # From local file\n        voice_state = model.get_state_for_audio_prompt(\"./my_voice.wav\")\n\n        # Reload state from a .safetensors file (much faster than extracting from an audio file)\n        voice_state = model.get_state_for_audio_prompt(\"./my_voices.safetensors\")\n\n        # From HTTP URL\n        voice_state = model.get_state_for_audio_prompt(\n            \"https://huggingface.co/kyutai/tts-voices/resolve\"\n            \"/main/expresso/ex01-ex02_default_001_channel1_168s.wav\"\n        )\n        ```\n\n    Note:\n        - Audio is automatically resampled to the model's sample rate (24kHz)\n        - The audio is encoded using the Mimi compression model and projected\n          to the flow model's latent space\n        - Processing time is logged for performance monitoring\n        - The state preserves speaker characteristics for voice cloning\n    \"\"\"\n    if isinstance(audio_conditioning, (str, Path)) and str(audio_conditioning).endswith(\n        \".safetensors\"\n    ):\n        if isinstance(audio_conditioning, str):\n            audio_conditioning = download_if_necessary(audio_conditioning)\n        import safetensors.torch\n\n        prompt = safetensors.torch.load_file(audio_conditioning)[\"audio_prompt\"]\n    elif isinstance(audio_conditioning, str) and audio_conditioning in PREDEFINED_VOICES:\n        # We get the audio conditioning directly from the safetensors file.\n        prompt = load_predefined_voice(audio_conditioning)\n    else:\n        if not self.has_voice_cloning and isinstance(audio_conditioning, (str, Path)):\n            raise ValueError(VOICE_CLONING_UNSUPPORTED)\n\n        if isinstance(audio_conditioning, str):\n            audio_conditioning = download_if_necessary(audio_conditioning)\n\n        if isinstance(audio_conditioning, Path):\n            audio, conditioning_sample_rate = audio_read(audio_conditioning)\n\n            if truncate:\n                max_samples = int(30 * conditioning_sample_rate)  # 30 seconds of audio\n                if audio.shape[-1] &gt; max_samples:\n                    audio = audio[..., :max_samples]\n                    logger.info(f\"Audio truncated to first 30 seconds ({max_samples} samples)\")\n\n            audio_conditioning = convert_audio(\n                audio, conditioning_sample_rate, self.config.mimi.sample_rate, 1\n            )\n\n        with display_execution_time(\"Encoding audio prompt\"):\n            prompt = self._encode_audio(audio_conditioning.unsqueeze(0).to(self.device))\n\n    model_state = init_states(self.flow_lm, batch_size=1, sequence_length=prompt.shape[1])\n\n    with display_execution_time(\"Prompting audio\"):\n        self._run_flow_lm_and_increment_step(model_state=model_state, audio_conditioning=prompt)\n\n    logger.info(\n        \"Size of the model state for audio prompt: %d MB\", size_of_dict(model_state) // 1e6\n    )\n\n    return model_state\n</code></pre>"},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel.load_model","title":"<code>load_model(config=DEFAULT_VARIANT, temp=DEFAULT_TEMPERATURE, lsd_decode_steps=DEFAULT_LSD_DECODE_STEPS, noise_clamp=DEFAULT_NOISE_CLAMP, eos_threshold=DEFAULT_EOS_THRESHOLD)</code>  <code>classmethod</code>","text":"<p>Load a pre-trained TTS model with specified configuration.</p> <p>This class method loads a complete TTS model including the flow language model and Mimi compression model from pre-trained weights. The model is initialized with the specified generation parameters and ready for inference.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str | Path</code> <p>a path to a custom YAML config file saved locally (e.g., C://pocket_tts/pocket_tts_config.yaml) or a model variant identifier (e.g., '610b0b2c'; must match a YAML file in the config directory).</p> <code>DEFAULT_VARIANT</code> <code>temp</code> <code>float | int</code> <p>Sampling temperature for generation. Higher values produce more diverse but potentially lower quality output.</p> <code>DEFAULT_TEMPERATURE</code> <code>lsd_decode_steps</code> <code>int</code> <p>Number of steps for Lagrangian Self Distillation decoding. More steps can improve quality but increase computation.</p> <code>DEFAULT_LSD_DECODE_STEPS</code> <code>noise_clamp</code> <code>float | int | None</code> <p>Maximum value for noise sampling. If None, no clamping is applied. Helps prevent extreme values in generation.</p> <code>DEFAULT_NOISE_CLAMP</code> <code>eos_threshold</code> <code>float</code> <p>Threshold for end-of-sequence detection. Higher values make the model more likely to continue generating.</p> <code>DEFAULT_EOS_THRESHOLD</code> <p>Returns:</p> Name Type Description <code>TTSModel</code> <code>Self</code> <p>Fully initialized model with loaded weights on cpu, ready for text-to-speech generation.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified config file or model weights are not found.</p> <code>ValueError</code> <p>If the configuration is invalid or incompatible.</p> Example <pre><code>from pocket_tts import TTSModel\n\n# Load with default settings\nmodel = TTSModel.load_model()\n\n# Load with custom parameters\nmodel = TTSModel.load_model(variant=\"b6369a24\", temp=0.5, lsd_decode_steps=5, eos_threshold=-3.0)\n</code></pre> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>@classmethod\ndef load_model(\n    cls,\n    config: str | Path = DEFAULT_VARIANT,\n    temp: float | int = DEFAULT_TEMPERATURE,\n    lsd_decode_steps: int = DEFAULT_LSD_DECODE_STEPS,\n    noise_clamp: float | int | None = DEFAULT_NOISE_CLAMP,\n    eos_threshold: float = DEFAULT_EOS_THRESHOLD,\n) -&gt; Self:\n    \"\"\"Load a pre-trained TTS model with specified configuration.\n\n    This class method loads a complete TTS model including the flow language model\n    and Mimi compression model from pre-trained weights. The model is initialized\n    with the specified generation parameters and ready for inference.\n\n    Args:\n        config: a path to a custom YAML config file saved locally (e.g., C://pocket_tts/pocket_tts_config.yaml)\n            or a model variant identifier (e.g., '610b0b2c'; must match a YAML file in the config directory).\n        temp: Sampling temperature for generation. Higher values produce more\n            diverse but potentially lower quality output.\n        lsd_decode_steps: Number of steps for Lagrangian Self Distillation\n            decoding. More steps can improve quality but increase computation.\n        noise_clamp: Maximum value for noise sampling. If None, no clamping\n            is applied. Helps prevent extreme values in generation.\n        eos_threshold: Threshold for end-of-sequence detection. Higher values\n            make the model more likely to continue generating.\n\n    Returns:\n        TTSModel: Fully initialized model with loaded weights on cpu, ready for\n            text-to-speech generation.\n\n    Raises:\n        FileNotFoundError: If the specified config file or model weights\n            are not found.\n        ValueError: If the configuration is invalid or incompatible.\n\n    Example:\n        ```python\n        from pocket_tts import TTSModel\n\n        # Load with default settings\n        model = TTSModel.load_model()\n\n        # Load with custom parameters\n        model = TTSModel.load_model(variant=\"b6369a24\", temp=0.5, lsd_decode_steps=5, eos_threshold=-3.0)\n        ```\n    \"\"\"\n    if str(config).endswith(\".yaml\"):\n        config_path = Path(config)\n        config = load_config(config_path)\n        logger.info(f\"Loading model from config at {config_path}...\")\n    else:\n        config = load_config(Path(__file__).parents[1] / f\"config/{config}.yaml\")\n\n    tts_model = TTSModel._from_pydantic_config_with_weights(\n        config, temp, lsd_decode_steps, noise_clamp, eos_threshold\n    )\n    return tts_model\n</code></pre>"},{"location":"API%20Reference/Reference/tts_model/#pocket_tts.models.tts_model.TTSModel.save_audio_prompt","title":"<code>save_audio_prompt(audio_conditioning, export_path, truncate=False)</code>","text":"<p>Save audio prompt to .safetensors file</p> <p>This method processes an audio prompt and exports it to a .safetensors file, which can be loaded by get_state_for_audio_prompt in subsequent uses without converting the audio again.</p> <p>It also takes an already converted audio tensor and exports it as a .safetensors file</p> <p>Parameters:</p> Name Type Description Default <code>audio_conditioning</code> <code>Path | str | Tensor</code> <p>Audio to export - Path: Local file path to audio file - str: URL to download audio file - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]</p> required <code>export_path</code> <code>Path | str</code> <p>Path to output file</p> required <code>truncate</code> <code>bool</code> <p>Whether to truncate long audio prompts to 30 seconds.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Audio tensor of converted audio</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If audio file path doesn't exist.</p> <code>ValueError</code> <p>If audio tensor export path is invalid or empty.</p> <code>RuntimeError</code> <p>If audio processing or encoding fails.</p> Example <pre><code>from pocket_tts import TTSModel\n\nmodel = TTSModel.load_model()\n# From HuggingFace URL\nmodel.get_state_for_audio_prompt(\n    \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\", \"casual.safetensors\"\n)\n\n# From local file (the .safetensors extension will be added automatically)\ntensor = model.get_state_for_audio_prompt(\"./my_voice.wav\", \"my_voice\")\n\n# Use the tensor, Luke!\naudio = model.generate_audio(tensor, \"Hello world!\")\n</code></pre> Note <ul> <li>Send resulting audio tensor to get_state_for_audio_prompt   in order to get the model state for generation.</li> </ul> Source code in <code>pocket_tts/models/tts_model.py</code> <pre><code>@torch.no_grad\ndef save_audio_prompt(\n    self,\n    audio_conditioning: Path | str | torch.Tensor,\n    export_path: Path | str,\n    truncate: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Save audio prompt to .safetensors file\n\n    This method processes an audio prompt and exports it to a .safetensors file,\n    which can be loaded by get_state_for_audio_prompt in subsequent uses\n    without converting the audio again.\n\n    It also takes an already converted audio tensor and exports it as a .safetensors file\n\n    Args:\n        audio_conditioning: Audio to export\n            - Path: Local file path to audio file\n            - str: URL to download audio file\n            - torch.Tensor: Pre-loaded audio tensor with shape [channels, samples]\n        export_path: Path to output file\n        truncate: Whether to truncate long audio prompts to 30 seconds.\n\n    Returns:\n        Audio tensor of converted audio\n\n    Raises:\n        FileNotFoundError: If audio file path doesn't exist.\n        ValueError: If audio tensor export path is invalid or empty.\n        RuntimeError: If audio processing or encoding fails.\n\n    Example:\n        ```python\n        from pocket_tts import TTSModel\n\n        model = TTSModel.load_model()\n        # From HuggingFace URL\n        model.get_state_for_audio_prompt(\n            \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\", \"casual.safetensors\"\n        )\n\n        # From local file (the .safetensors extension will be added automatically)\n        tensor = model.get_state_for_audio_prompt(\"./my_voice.wav\", \"my_voice\")\n\n        # Use the tensor, Luke!\n        audio = model.generate_audio(tensor, \"Hello world!\")\n        ```\n\n    Note:\n        - Send resulting audio tensor to get_state_for_audio_prompt\n          in order to get the model state for generation.\n    \"\"\"\n    if not export_path or not isinstance(export_path, (str, Path)):\n        raise ValueError(\"export_path must be of type str or Path\")\n    export_path = Path(export_path).with_suffix(\".safetensors\")\n\n    if not self.has_voice_cloning and isinstance(audio_conditioning, (str, Path)):\n        raise ValueError(VOICE_CLONING_UNSUPPORTED)\n\n    if isinstance(audio_conditioning, str):\n        audio_conditioning = download_if_necessary(audio_conditioning)\n\n    if isinstance(audio_conditioning, Path):\n        audio, conditioning_sample_rate = audio_read(audio_conditioning)\n\n        if truncate:\n            max_samples = int(30 * conditioning_sample_rate)  # 30 seconds of audio\n            if audio.shape[-1] &gt; max_samples:\n                audio = audio[..., :max_samples]\n                logger.info(f\"Audio truncated to first 30 seconds ({max_samples} samples)\")\n\n        audio_conditioning = convert_audio(\n            audio, conditioning_sample_rate, self.config.mimi.sample_rate, 1\n        )\n\n    with display_execution_time(\"Exporting audio prompt\"):\n        prompt = self._encode_audio(audio_conditioning.unsqueeze(0).to(self.device))\n        import safetensors.torch\n\n        safetensors.torch.save_file({\"audio_prompt\": prompt}, export_path)\n\n    return audio_conditioning\n</code></pre>"},{"location":"CLI%20Commands/export_voice/","title":"Export Voice","text":"<p>Kyutai Pocket TTS allows you to generate speech from an audio sample. However, processing an audio file each time is relatively slow and inefficient.</p> <p>The <code>export-voice</code> command allows you to convert an audio file to a voice embedding in safetensors format. The safetensors file can then be loaded very quickly whenever you generate speech.</p>"},{"location":"CLI%20Commands/export_voice/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts export-voice audio-path export-path\n# or if installed manually:\npocket-tts export-voice audio-path export-path\n</code></pre>"},{"location":"CLI%20Commands/export_voice/#command-options","title":"Command Options","text":""},{"location":"CLI%20Commands/export_voice/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>audio-path</code>: Path of the audio file or directory to convert. <code>audio-path</code> can point to an <code>http:</code> or <code>hf:</code> (hugging face) file. If <code>audio-path</code> is a local directory, all audio files found inside it will be batch converted. Supports popular audio file formats like wav and mp3.</li> <li><code>export-path</code>: Path of safetensors file or directory to export to. For batch conversion, export-path should be a directory. The directory will be created if it does not exist.</li> </ul>"},{"location":"CLI%20Commands/export_voice/#options","title":"Options","text":"<ul> <li><code>--truncate</code>: Automatically truncate long audio files down to 30 seconds.</li> </ul> <p>The other parameters such as <code>--lsd-decode-steps</code> and <code>--temperature</code> are the same as for the <code>generate</code> command. See the generate documentation for more details.</p>"},{"location":"CLI%20Commands/export_voice/#examples","title":"Examples","text":"<pre><code># export a single file\npocket-tts export-voice voice_memo127762.mp3 jack.safetensors\n\n# export a single file to a different directory (output is embbeddings/mary.safetensors\npocket-tts export-voice voices/mary.wav embeddings/\n\n# export an entire directory of audio files, truncate long audios\npocket-tts export-voice voices/ embeddings/ --truncate\n\n# export an online file to current directory\npocket-tts export-voice https://huggingface.co/kyutai/tts-voices/resolve/main/alba-mackenna/announcer.wav .\n\n# use the exported safetensors\npocket-tts generate --text \"Hello, welcome to today's game between the Bears and Cubs.\"  --voice announcer.safetensors\n</code></pre> <p>Note: to indicate a directory rather than a file, please be sure to include a trailing / (\\ on Windows).</p>"},{"location":"CLI%20Commands/generate/","title":"Generate","text":"<p>The <code>generate</code> command allows you to generate speech from text directly from the command line using Kyutai Pocket TTS.</p>"},{"location":"CLI%20Commands/generate/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts generate\n# or if installed manually:\npocket-tts generate\n</code></pre> <p>This will generate a WAV file <code>./tts_output.wav</code> with the default text and voice.</p>"},{"location":"CLI%20Commands/generate/#command-options","title":"Command Options","text":""},{"location":"CLI%20Commands/generate/#core-options","title":"Core Options","text":"<ul> <li><code>--text TEXT</code>: Text to generate (default: \"Hello world! I am Kyutai Pocket TTS. I'm fast enough to run on small CPUs. I hope you'll like me.\")</li> <li><code>--voice VOICE</code>: Path to audio conditioning file (voice to clone) (default: \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\"). Urls and local paths are supported.</li> <li><code>--output-path OUTPUT_PATH</code>: Output path for generated audio (default: \"./tts_output.wav\")</li> </ul>"},{"location":"CLI%20Commands/generate/#generation-parameters","title":"Generation Parameters","text":"<ul> <li><code>--config CONFIG_PATH</code>: Path to custom config.yaml (for loading local model files) or model signature (default: \"b6369a24\")</li> <li><code>--lsd-decode-steps LSD_DECODE_STEPS</code>: Number of generation steps (default: 1)</li> <li><code>--temperature TEMPERATURE</code>: Temperature for generation (default: 0.7)</li> <li><code>--noise-clamp NOISE_CLAMP</code>: Noise clamp value (default: None)</li> <li><code>--eos-threshold EOS_THRESHOLD</code>: EOS threshold (default: -4.0)</li> <li><code>--frames-after-eos FRAMES_AFTER_EOS</code>: Number of frames to generate after EOS (default: None, auto-calculated based on the text length). Each frame is 80ms.</li> </ul>"},{"location":"CLI%20Commands/generate/#performance-options","title":"Performance Options","text":"<ul> <li><code>--device DEVICE</code>: Device to use (default: \"cpu\", you may not get a speedup by using a gpu since it's a small model)</li> <li><code>--quiet</code>, <code>-q</code>: Disable logging output</li> </ul>"},{"location":"CLI%20Commands/generate/#examples","title":"Examples","text":""},{"location":"CLI%20Commands/generate/#basic-generation","title":"Basic Generation","text":"<pre><code># Generate with default settings\npocket-tts generate\n\n# Custom text\npocket-tts generate --text \"Hello, this is a custom message.\"\n\n# Custom output path\npocket-tts generate --output-path \"./my_audio.wav\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#voice-selection","title":"Voice Selection","text":"<pre><code># Use different voice from HuggingFace\npocket-tts generate --voice \"hf://kyutai/tts-voices/jessica-jian/casual.wav\"\n\n# Use local voice file\npocket-tts generate --voice \"./my_voice.wav\"\n\n# Use a safetensors file (such as one created using `pocket-tts export-voice`)\npocket-tts generate --voice \"./my_voice.safetensors\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#quality-tuning","title":"Quality Tuning","text":"<pre><code># Higher quality (more steps)\npocket-tts generate --lsd-decode-steps 5 --temperature 0.5\n\n# More expressive (higher temperature)\npocket-tts generate --temperature 1.0\n\n# Adjust EOS threshold, smaller means finishing earlier.\npocket-tts generate --eos-threshold -3.0\n</code></pre>"},{"location":"CLI%20Commands/generate/#custom-model-config","title":"Custom Model Config","text":"<p>If you'd like to override the paths from which the models are loaded, you can provide a custom YAML configuration.</p> <p>Copy pocket_tts/config/b6369a24.yaml and change weights_path:, weights_path_without_voice_cloning: and tokenizer_path: to the paths of the models you want to load.</p> <p>Then, use the --config option to point to your newly created config.</p> <pre><code># Use a different config\npocket-tts generate --config \"C://pocket-tts/my_config.yaml\"\n</code></pre>"},{"location":"CLI%20Commands/generate/#output-format","title":"Output Format","text":"<p>The generate command always outputs WAV files in the following format:</p> <ul> <li>Sample Rate: 24kHz</li> <li>Channels: Mono</li> <li>Bit Depth: 16-bit PCM</li> <li>Format: Standard WAV file</li> </ul> <p>For more advanced usage, see the Python API documentation or consider using the serve command for web-based generation and quick iteration.</p>"},{"location":"CLI%20Commands/serve/","title":"Serve","text":"<p>The <code>serve</code> command starts a FastAPI web server that provides both a web interface and HTTP API for text-to-speech generation.</p>"},{"location":"CLI%20Commands/serve/#basic-usage","title":"Basic Usage","text":"<pre><code>uvx pocket-tts serve\n# or if installed manually:\npocket-tts serve\n</code></pre> <p>This starts a server on <code>http://localhost:8000</code> with the default voice model.</p>"},{"location":"CLI%20Commands/serve/#command-options","title":"Command Options","text":"<ul> <li><code>--voice VOICE</code>: Path to voice prompt audio file (voice to clone) (default: \"hf://kyutai/tts-voices/alba-mackenna/casual.wav\")</li> <li><code>--host HOST</code>: Host to bind to (default: \"localhost\")</li> <li><code>--port PORT</code>: Port to bind to (default: 8000)</li> <li><code>--reload</code>: Enable auto-reload for development</li> <li><code>--config</code>: Path to a custom config .yaml</li> </ul>"},{"location":"CLI%20Commands/serve/#examples","title":"Examples","text":""},{"location":"CLI%20Commands/serve/#basic-server","title":"Basic Server","text":"<pre><code># Start with default settings\npocket-tts serve\n\n# Custom host and port\npocket-tts serve --host \"localhost\" --port 8080\n</code></pre>"},{"location":"CLI%20Commands/serve/#custom-voice","title":"Custom Voice","text":"<pre><code># Use different voice\npocket-tts serve --voice \"hf://kyutai/tts-voices/jessica-jian/casual.wav\"\n\n# Use local voice file\npocket-tts serve --voice \"./my_voice.wav\"\n</code></pre>"},{"location":"CLI%20Commands/serve/#custom-model-config","title":"Custom Model Config","text":"<p>If you'd like to override the paths from which the models are loaded, you can provide a custom YAML configuration.</p> <p>Copy pocket_tts/config/b6369a24.yaml and change weights_path:, weights_path_without_voice_cloning: and tokenizer_path: to the paths of the models you want to load.</p> <p>Then, use the --config option to point to your newly created config.</p> <pre><code># Use a different config\npocket-tts serve --config \"C://pocket-tts/my_config.yaml\"\n</code></pre>"},{"location":"CLI%20Commands/serve/#web-interface","title":"Web Interface","text":"<p>Once the server is running, navigate to <code>http://localhost:8000</code> to access the web interface.</p> <p>For more advanced usage, see the Python API documentation for direct integration with the TTS model.</p>"}]}